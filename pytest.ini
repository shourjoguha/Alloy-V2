[pytest]
asyncio_mode = auto
filterwarnings =
    ignore::DeprecationWarning
    ignore::UserWarning

# pytest-benchmark configuration
markers =
    benchmark: mark test as a benchmark
benchmark:
    # Benchmark storage location
    storage = .benchmarks
    # Number of calibration rounds before measuring
    calibration_rounds = 2
    # Timer to use (time.process_time, perf_counter, wall_time)
    timer = perf_counter
    # Disable garbage collection during benchmark
    disable_gc = true
    # Minimum number of iterations
    min_rounds = 5
    # Maximum time to spend on a single benchmark (seconds)
    max_time = 60
    # Save baseline results
    save = true
    # Automatically save data to storage
    autosave = true
    # JSON output file for benchmark results
    json_file = benchmark_results.json

# Test paths
testpaths = tests
